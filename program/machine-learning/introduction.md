# 机器学习 - 介绍

> [!tip] 快速入门
> 
> 推荐课程：[李宏毅2023春机器学习课程](https://www.bilibili.com/video/BV1Wv411h7kN)

## 引入

1. `ChatGPT` 实现基本原理：文字接龙；

    `BERT` 实现的基本原理是：文字填空。

2. 预训练模型→下游模型

    预训练模型（基石模型）：自督导式学习。

    下游模型：督导式学习→增强式学习（由人类干涉）。

> [!tip] 提示
> `ChatGPT`：自督导式→督导式学习→增强式学习。
> 
> 自督导式：学会文字接龙；
>
> 督导式学习：学会正确的文字接龙；
>
> 增强式学习：学会更好的文字接龙。

## 基础

1. 机器学习 ≈ 机器自动找到一个函式。
2. 根据函式的输出分类：

    - 回归：函式的输出是一个数值；
    - 分类：函式的输出是一个类别。

> [!tip] 提示
> 
> `ChatGPT`：属于分类，把生成式学习拆解成多个分类问题。

1. 找函式的三个步骤：

    - 设定范围 —— 定出候选函式的集合（Model）；
    - 设定标准 —— 定出【评量函式好坏】的标准（Loss）；
    - 达成目标 —— 找出最好的函式。

> [!tip] 提示
> 
> `CNN,RNN,Transformer(类神经网络结构)` 就是候选函式的集合。

## 生成式学习

### 两种策略

> 各个击破、一次到位。

1. 各个击破：

    - 速度较慢：一次只生成一个元素，每个元素都要等前一个元素生成。
    - 品质更好。

2. 一次到位：

    - 速度快：只要有足够的平行算力，就可以完全生成。
    - 品质较差。

推荐：各个击破 + 一次到位

- 以各个击破为基础，再一次到位；
- N次到位。

### 两种不同期待

1. 成为通才：适用于各种任务，可以快速开发出新的功能；
2. 成为专才：在单一任务上会比通才较好。

### 对预训练的模型进行改造

1. 微调：把模型的原来参数当作初始化参数，使用梯度下降法对模型进行微调。
2. Adapter(适配器)：不改动模型的参数，为模型提供额外的模组（适应更多的任务）。

## In-context Learning

> 情境学习

为机器提供标注的范例进行“学习” —— 模型其实本来就会情境分析，只是需要被指出要做情境任务。

> 提供过多的错误示例，可能会导致模型自动学习到错误的知识。

## Instruction-tuning

> 微调

激发语言模型 的理解能力，它通过给出更明显的指令，让模型去做出正确的行动。指示学习的优点是它经过多任务的微调后，也能够在其他任务上做 `zero-shot`。

## Chain of Thought （CoT）

思维链(`Chain-of-thought`，`CoT`)是指令示范的一种特殊情况，它通过引发对话代理的逐步推理来生成输出。 使用 `CoT` 微调的模型使用带有逐步推理的人工标注的指令数据集。

## 大模型+大资料

> 产生质的变化，模型会突然顿悟。

### 大模型的顿悟时刻

模型的效果不是随着模型参数量变多而慢慢变好，而是在某一个瞬间，模型“顿悟”了

### 大资料-两种认知

语言模型一般需要两种认知

- 一种是对语言本身语法、文法的认知
    - 这一部分不需要很多的语料，有一些资料供学习就够了
- 另一种是对语言背后世界运行规律的认知
    - 这一部分就需要大量的语料喂入了

### 大资料筛选

- 过滤有害内容
- 去除无关资料
- 用规则去除低品质的资料
- 去除重复的资料
- 要及时去除测试资料

### KNN LM

`knn` 又称为K临近分类算法。简单地说，`K`-近邻算法采用测量不同特征值之间的距离方法进行分类（`k-Nearest Neighbor`，`KNN`）。“物以类聚，人以群分”，这是这个算法的一个思想。近邻算法，即是给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的K个实例，这K个实例的多数属于某个类，就把该输入实例分类到这个类中。

## 图像生成

> 图像版 GTP

### 各个击破

一个一个像素地生成，耗时过长。

### 一次到位

一次性生成一张图片。

## Stable Diffusion / Imagen

文字模型 --> 中间产物（图片的压缩版本） --> 还原成原来图片
